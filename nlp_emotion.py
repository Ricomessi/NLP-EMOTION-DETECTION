# -*- coding: utf-8 -*-
"""NLP EMOTION.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15r1J2qgbpe0T_eD9cv_sbuK9sno_a_CA

### **Import library yang digunakan**
"""

import re
import nltk
import pickle
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report
from nltk.corpus import stopwords
import tensorflow as tf
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('wordnet')

"""### **Reading data**"""

data = pd.read_csv("/content/text.csv")
data.drop(columns="Unnamed: 0", inplace=True)
data.head()

"""### **Data reading**"""

data.info()

"""### **Label classification**"""

data['label'] = data['label'].replace('suppride', 'surprise')
emotion = {
    0: "sadness",
    1: "joy",
    2: "love",
    3: "anger",
    4: "fear",
    5: "suppride"
}
data["label"] = data["label"].map(emotion)

"""### **Preprocessing**"""

data.duplicated().sum()

data_dupe = data[data.duplicated()]
print("data duplikat: ")
data_dupe

data = data.drop_duplicates(keep='first')
data.shape

data['label'].value_counts()

# Check for NaN values in the 'text' and 'label' columns
print(data.isna().sum())

# Drop rows with NaN values in 'text' or 'label' columns
data = data.dropna(subset=['text', 'label'])

# Check for NaN values in the 'text' and 'label' columns
print(data.isna().sum())

# Drop rows where 'text' or 'label' is NaN
data = data.dropna(subset=['text', 'label'])

# Check if there are any missing values left
print(data.isna().sum())

stop_words = set(stopwords.words('english'))

def remove_stopwords(text):
    words = word_tokenize(text)
    filtered_words = [word for word in words if word.lower() not in stop_words]
    return " ".join(filtered_words)

data['text'] = data['text'].apply(remove_stopwords)
data.head()

print(len(data))

lemmatizer = WordNetLemmatizer()
data['text'] = data['text'].apply(lambda x: ' '.join(
    [lemmatizer.lemmatize(word) for word in word_tokenize(re.sub(r'[^\w\s]', '', x).lower()) if word]
))
data.head()

"""### **Splitting data**"""

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data['text'], data['label'], test_size=0.25, random_state=42, stratify=data['label'])

# Display the shapes of the training and testing sets
print("\nShapes of Training and Testing Sets:")
print("X_train:", X_train.shape)
print("X_test:", X_test.shape)
print("y_train:", y_train.shape)
print("y_test:", y_test.shape)

"""### **Modelling**"""

# Simplified label mapping and data preprocessing
label_mapping = {v: k for k, v in emotion.items()}

y_train_encoded = y_train.map(label_mapping)
y_test_encoded = y_test.map(label_mapping)

# Convert to TensorFlow datasets
train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train_encoded)).batch(64)
test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test_encoded)).batch(64)

# Create a simplified text vectorization layer
vectorize_layer = tf.keras.layers.TextVectorization(
    max_tokens=5000,
    output_mode='int',
    output_sequence_length=50
)

# Adapt the vectorization layer to the training data
vectorize_layer.adapt(X_train)

model = tf.keras.Sequential([
    vectorize_layer,
    tf.keras.layers.Embedding(input_dim=len(vectorize_layer.get_vocabulary()), output_dim=32),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),
    tf.keras.layers.Dense(len(label_mapping), activation='softmax')  # Output layer
])

# Compile the model
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model with fewer epochs
history = model.fit(
    train_dataset,  # Already batched in the dataset
    epochs=5,
    validation_data=test_dataset,
)

# Evaluate the model
loss, accuracy = model.evaluate(test_dataset)
print("Loss:", loss)
print("Accuracy:", accuracy)

"""### **Saving Data**"""

import pathlib
export_dir = 'saved_model/'
tf.saved_model.save(model, export_dir)

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epochs')
plt.legend(['train', 'test'], loc = 'upper right')
plt.show()

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['train', 'test'], loc='lower right')
plt.show()

"""### **Cllasification Report**"""

y_pred = model.predict(test_dataset)
y_pred_classes = np.argmax(y_pred, axis=1)

print(classification_report(y_test_encoded, y_pred_classes))